#################################################################################################################################

#GENERAL
###All lines in this file starting with # will be ignored by Python.
###Read the [DEFAULT] section on how to use variables in this file. This is IMPORTANT!
###Some variables in DEFAULT section is commented out intentionally to avoid Python picking up default values inadvertently.
###DO NOT change [DEFAULT] section unless you're very sure, as these are the default values for all following sections.
###This config file should be saved in the same folder as dataInterface.py and diCaller.py

#VARIABLES
###Order of variables doesn't matter.
###But variable names must be same as in sections below. 
###Variable names are always all lower case (Eg: log_file_dir is correct, Log_File_Dir is wrong, LOG_FILE_DIR is wrong).
###No variables or their values should be enclosed in quotes.
###Our AIX has some limitations so all passwords/access keys must be provided in plain text. On Linux/Windows it should be encrypted.
###All filenames in diConfig.ini should have the absolute path, but again, read comments in [DEFAULT] section.

#SECTIONS
###Create your own section (like [BIOSYENT]) for a new project.
#You will pass the section name as a parameter to the calling module (diCaller.py).
###All data loading happens under a single S3 bucket(many folders are possible). To load to a separate S3 bucket, create a new section. 
#The new section will have all usuall variables in a section, along with those variables for the new bucket,folders, files etc. 
###Create a new section if you want to:
#load to a second bucket
#have a different run schedule(mnthly/wkly)
#have a different environment(dev/prod)
#For example:
#[BIOSYENT.MNTHLY] for monthly runs and [BIOSYENT.WKLY] for weekly runs. You're free to choose the string that goes
#inside [], for example [PFIZER_PROD--DAILY], but pass that string as parameter to the calling script (diCaller.py) as shown below.

#USAGE
#		python <calling_module_name.py> <section_name_in_diConfig.ini>
#Eg:	python diCaller.py BIOSYENT.WKLY

#####################################################################################################################################


[DEFAULT]
#log_file_dir: Absolute path of log directory. Don't provide a file name. Don't provide path delimiter at the end.
log_file_dir = full-path-for-log-directory---no-filenames---no-slash-at-the-end

#key_file_name: Key file name with absolute path. This file contains the key that's used to encrypt/decrypt passwords. Key is a random text generated by executing Fernet.generate_key().
key_file_name = file-name-with-full-path

#s3_region_name: Note that Amazon charges for cross-region data transfers between AWS services.
s3_region_name = us-east-2

#s3_automatic_multipart_upload: If set to true, automatically initiates multipart upload into S3 when file size is above 100MB. 
#Multipart upload splits large files into many chunks and loads them in parallel.
s3_automatic_multipart_upload = true

#s3_bucket_name: If you want some of the files to go in a separate bucket, create a separate configuration section (eg: [BIOSYENT_2]), copy paste all variable
#names and values under the new section along with new bucket name. But include only those SQL statements, SQL output file names and S3 folder names that you 
#want to go into this bucket. Pass the new section name to the calling module (diCaller.py) to load data into this bucket
s3_bucket_name = bucket-here

#s3_storage_class: Options (as of 2018) are STANDARD|REDUCED_REDUNDANCY|STANDARD_IA|ONEZONE_IA|INTELLIGENT_TIERING|GLACIER
s3_storage_class = STANDARD

#aws_access_key_id: Provide AWS access key id
aws_access_key_id = AKIAJGA3YXWZ3U2IDXIQ

#aws_secret_access_key: Call dataInterface.encryptData() to encrypt a password. If this is AIX, the actual password in plain text needs to be provided. Provide encrypted password on Windows/Linux.
aws_secret_access_key = encrypted-secret-access-key-goes-here

#sql_stmt_1, outputfile_of_sql_stmt_1, s3_folder_name_1 are commented out in DEFAULT section to avoid Python indavertenlty picking up these values from DEFAULT 
#section just in case a new section (like BIOSYENT.PROD) does not have _1 variables and begin at _2, _3 and so on.

#IMPORTANT: These 3 variables are related: sql_stmt_1, outputfile_of_sql_stmt_1, s3_folder_name_1. That is, output of sql_stmt_1 will go to file 
#outputfile_of_sql_stmt_1 and this file will be loaded in S3 folder s3_folder_name_1. For example output of sql_stmt_1 won't be written into outputfile_of_sql_stmt_2 or
#into s3_folder_name_3. If you want many files to go into one S3 folder, each file and its respective folder name have to be still specified separately.

#s3_folder_name_N: Give the absolute path starting from parent directory in s3. Do not include bucket name. Do not add \ or / at the end.
#Filename mentioned in outputfile_of_sql_stmt_1 will go to s3_folder_name_1; outputfile_of_sql_stmt_2 will go to s3_folder_name_2 and so on..
#If more than one file needs to go in the same S3 folder, just repeat folder name variable - for instance, to put both test_1.dat & test_2.dat into S3 folder myfolder:
#s3_folder_name_1=myfolder	s3_folder_name_2=myfolder	outputfile_of_sql_stmt_1=/home/imcadm/test_1.dat	outputfile_of_sql_stmt_2=/home/imcadm/test_2.dat
#s3_folder_name_1 = Folder1/Folder2

#sql_stmt_N: Holds SQL statment. If there's only one SQL statement use sql_stmt_1 (and similarly outputfile_of_sql_stmt_1, and s3_folder_name_1)
#sql_stmt_1..N variables are mutually exclusive with the setting oracle_spooling = true. That is, when spooling is true, output is already spooled, there is no SQL to run.
#sql_stmt_1 = SELECT 1 FROM dual

#outputfile_of_sql_stmt_N: Resultset of sql_stmt_number_N will be written to the file mentioned in this variable. 
#This filename (minus the path) will be the S3 key.
#File is overwritten into S3 on each run.
#This file has to be in the same server as this config file and Python modules. Location in the server doesn't matter.
#sql_stmt_1's output will go to outputfile_of_sql_stmt_1; sql_stmt_2's output will go to outputfile_of_sql_stmt_2, and so on..
#outputfile_of_sql_stmt_1 = C:\Users\rajesh.samuel\Documents\dim_date.csv

#outputfile_format_delimiter: Values are
# , (for csv) 
# | (for pipe delimited) etc. Not applicable with oracle_sqlplus_connection = true or with oracle_spooling = true
outputfile_format_delimiter = ,

#outputfile_format_quote: Possible values are 
#QUOTE_ALL (quote with "" on all fields), 
#QUOTE_MINIMAL ("" only on fields that contain these characters: outputfile_format_delimiter or outputfile_format_quote or spl. chars), 
#QUOTE_NONNUMERIC ("" only on all non-numeric fields, 
#QUOTE_NONE (no quotes). 
#Not applicable with oracle_sqlplus_connection = true or with oracle_spooling = true
outputfile_format_quote = QUOTE_NONE

#outputfile_format_escapechar: When file_format_quote is QUOTE_NONE Python expects an escape character (like a regex escape) whenever data has a spl. char or delimiter or quote as value
#Not applicable with oracle_sqlplus_connection = true or with oracle_spooling = true
outputfile_format_escapechar = \

#outputfile_format_header: Set to true to add a header row with column names
outpufile_format_header = true

#oracle_spooling: Set to true if data file to be loaded to AWS is generated by Oracle via PL/SQL instead of Python sending SQLs to Oracle. If set to true,
#variables sql_stmt_1..N and outputfile_format_.. should not be specified (will be ignored if specified).
oracle_spooling = false

#oracle_sqlplus_connection: Connects to Oracle via SQLPlus when set to true instead of via cx_Oracle pkg. This is used to get around old AIX limitations with cx_Oracle.
#Must be set to true on AIX! When set to true, tilde(~) separated fields with no quotes is the only formatting option (can be changed via simple code change)
oracle_sqlplus_connection = false

oracle_service_name = OracleServiceName From TNSNames.Ora GoesHere
oracle_user_name = OracleUserNameGoesHere

#oracle_password: Call dataInterface.encryptData() to encrypt a password. If this is AIX, the actual password in plain text needs to be provided. Provide encrypted password on Windows/Linux.
oracle_password = EncryptedOraclePasswordGoesHere

#s3_file_compress: Set to true to compress to gzip format before loading to S3. Do this! Bezos charges for bytes.
s3_file_compress = true

#s3_backup: Set to true to back up data files into an S3 location.
#To query backup data simply create a new external table in Athena or Spectrum (with table name as, for example, fact_tsa_2017_12) and point
#it to the backup S3 folder (eg: backup/2017/12/fact_tsa). If s3_file_compress=true, the compressed files will be backed up. Compression is recommended.
s3_backup = true

s3_backup_bucket_name = s3-bkps-dev

#s3_backup_basefolder_name: Parent folder for backups. This folder must be immediately under s3_backup_bucket_name.
s3_backup_basefolder_name = backup

#s3_backup_storage_class: Options (as of 2018) are STANDARD|REDUCED_REDUNDANCY|STANDARD_IA|ONEZONE_IA|INTELLIGENT_TIERING|GLACIER
s3_backup_storage_class = STANDARD

#local_backup: Set to true to backup data files on the local server (usually the Oracle server). Local backups are always compressed (gzip).
local_backup = false

#local_backup_basefolder_name: Parent folder for backups. Provide the absolute path. The program will create subfolders under this folder based on data month.
local_backup_basefolder_name = /home/imcadm/biosyent/data

#folder2folder_copy: When set to true, recursively copies all files and sub folders from a local folder or EC2 folder to an S3 folder.
#This is useful to backup large number of program files or log files from EC2 or an on-prem machine to S3
folder2folder_copy = false 

#folder2folder_source_folder: Absolute path of source folder whose contents are to be copied to S3. All files and sub-folders will be copied as is.
folder2folder_source_folder = /home/imcadm/biosyent/data

#folder2folder_target_s3_basefolder: Folder name in S3 where contents of source folder should land. Do not include bucket name. Under this folder, Python will create data month 
#subfolders, and then the source folder tree under data month subfolders. S3 settings provided in the section (like compression, bucket name, storage class etc.) applies here.
folder2folder_target_s3_basefolder = backup/misc


[BIOSYENT.DEV]
log_file_dir = /home/imcadm/biosyent/logs
s3_region_name = us-east-2
s3_bucket_name = bio
aws_access_key_id = AKIAJGA3YXWGEGHJADXIQdelete
aws_secret_access_key = gAAAAABb_E__mMpFNjKJOIJOI80aObW4KALWHI72T6UG6GyYk9UtF7PDyKfYHHMAUNCGcJq-UbVlArjVTWM7qF5UChqd5OL2$huYil
s3_file_compress = true
s3_backup = true
s3_backup_bucket_name = biosyent
s3_backup_basefolder_name = backup
s3_backup_storage_class = STANDARD_IA
key_file_name = /home/im/BIO/keyfile.key
oracle_spooling = false
oracle_sqlplus_connection = true
oracle_service_name = IMDWH
oracle_user_name = BTS_12
oracle_password = gAAAAABb_H1083YBbXrfw9KLJv4HcJNItmWbM3WfgBaYp9zcEf1xkuTSbCPx86djqUW_EHbI5OsT5b4bE5vUPMMy6GwQHGXQ==
#outputfile_format_delimiter = ,
#outputfile_format_quote = QUOTE_NONE
#outputfile_format_escapechar = \
outpufile_format_header = true
sql_stmt_1 = SELECT SYSDATE FROM DUAL
